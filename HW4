import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.linear_model import Ridge
from sklearn import metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.svm import SVC, SVR
from sklearn.decomposition import PCA as RandomizedPCA
from sklearn.compose import ColumnTransformer
from sklearn.metrics import mean_squared_error, r2_score

data = 'cancer.csv'
cancer = pd.DataFrame(pd.read_csv(data))
cancer.head(10)

features = cancer.iloc[:,2:30].to_numpy()
labels = cancer.iloc[:, 1]
features[:5]

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=0)

scaler = StandardScaler()
X_train_sc = scaler.fit_transform(X_train)
X_test_sc = scaler.fit_transform(X_test)

max_components = X_train.shape[1]

optimal_values = {'kernel': '', 'components': 0, 'accuracy': 0}

kernels = ['linear', 'rbf', 'poly', 'sigmoid']
results = {}

for kernel in kernels:
    results[kernel] = {'accuracy': [], 'precision': [], 'recall': []}

for components in range(1, max_components + 1):
    pca = PCA(n_components=components)
    X_train_pca = pca.fit_transform(X_train_sc)
    X_test_pca = pca.transform(X_test_sc)

    for kernel in kernels:
        svm = SVC(kernel=kernel)
        svm.fit(X_train_pca, Y_train)

        Y_pred = svm.predict(X_test_pca)

        accuracy = accuracy_score(Y_test, Y_pred)
        precision = (precision_score(Y_test, Y_pred, pos_label='M'))
        recall = (recall_score(Y_test, Y_pred, pos_label='M'))

        results[kernel]['accuracy'].append(accuracy)
        results[kernel]['precision'].append(precision)
        results[kernel]['recall'].append(recall)

if accuracy > optimal_values['accuracy']:
    optimal_values['accuracy'] = accuracy
    optimal_values['kernel'] = kernel
    optimal_values['components'] = components

print(f"Optimal kernel: {optimal_values['kernel']}")
print(f"Optimal Number of Principal Components: {optimal_values['components']}")
print(f"Highest Classification Accuracy: {optimal_values['accuracy']}")

metrics = ['accuracy', 'precision', 'recall']
titles = ['Accuracy', 'Precision', 'Recall']

for idx, metric in enumerate(metrics):
    plt.figure(figsize=(10, 6))
    for kernel in kernels:
        plt.plot(range(1, max_components + 1), results[kernel][metric], label=f'{kernel} Kernel')
    plt.title(f'{titles[idx]} for Different Kernel Tricks vs. Principal Components')
    plt.xlabel('Number of Principal Components')
    plt.ylabel(titles[idx])
    plt.legend()
    plt.grid(True)
    plt.show()

housing_data = 'Housing.csv'
housing = pd.DataFrame(pd.read_csv(housing_data))
housing.head(10)

features = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']

def binary_mapping(x):
    return x.map({'yes': 1, 'no': 0})

housing[features] = housing[features].apply(binary_mapping)
housing.head()

features = ['area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'parking', 'prefarea']
X1 = housing[features].values
Y1 = housing[['price']].values.reshape(-1,1)
X1[:5]

X1_train, X1_test, Y1_train, Y1_test = train_test_split(X1, Y1, test_size = 0.20, random_state = 0)
sc_X = StandardScaler()
X1_train_sc = sc_X.fit_transform(X1_train)
X1_test_sc = sc_X.fit_transform(X1_test)
X1_test.shape

accuracy = []
optimal_k = 0
lowest_mse = float('inf')

for n_components in range(1, X1_train.shape[1]+1):
    for k in range(1, X1_train.shape[1]+1):
        pca = PCA(n_components = n_components)
        X1_train_pca = pca.fit_transform(X1_train_sc)
        X1_test_pca = pca.transform(X1_test_sc)

        svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)
        svr_linear = SVR(kernel = 'linear', C=1e3)
        svr_poly = SVR(kernel = 'poly', C=1e3, degree = 2)

        svr_rbf.fit(X1_train_pca, Y1_train)
        svr_linear.fit(X1_train_pca, Y1_train)
        svr_poly.fit(X1_train_pca, Y1_train)

        y_rbf = svr_rbf.predict(X1_test_pca)
        y_linear = svr_linear.predict(X1_test_pca)
        y_poly = svr_poly.predict(X1_test_pca)

        svr = SVR(kernel=kernel)
        svr.fit(X1_train, Y1_train)

        Y1_predictions = svr.predict(X1_test)

        mse = mean_squared_error(Y1_test, Y1_predictions)
        accuracy.append(mse)

if mse < lowest_mse:
    lowest_mse = mse
    optimal_k = k

print(f"Number of Principal Componenets (K): {n_components}, K Value: {k}, Accuracy (MSE): {mse}")
print(f"Best K Value: {optimal_k}, Best Accuracy (MSE): {lowest_mse}")

plt.figure(figsize=(13, 5))

plt.subplot(1, 3, 1)
plt.scatter(y_linear, Y1_test, color='darkorange', label='test data')
plt.xlabel('Data')
plt.ylabel('Price')
plt.title('Linear SVR')

plt.subplot(1, 3, 2)
plt.scatter(y_poly, Y1_test, color='blue', label='test data')
plt.xlabel('Data')
plt.ylabel('Price')
plt.title('Poly SVR')

plt.subplot(1, 3, 3)
plt.scatter(y_rbf, Y1_test, color='red', label='test data')
plt.xlabel('Data')
plt.ylabel('Price')
plt.title('RBF SVR')

plt.tight_layout()
plt.show()
